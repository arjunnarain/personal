---
title: "BERT: Pre-training of Deep Bidirectional Transformers"
date: 2025-08-31
draft: false
tags: ["bert", "transformer", "nlp", "pretraining"]
categories: ["research-papers"]
reading_status: ["currently-reading"]
author: "Devlin et al."
journal: "NAACL"
year: "2019"
doi: ""
paper_url: "https://arxiv.org/abs/1810.04805"
drive_link: "#"
summary: "BERT introduces bidirectional training for language models, achieving state-of-the-art results across multiple NLP tasks."
ShowToc: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowShareButtons: true
---

## Abstract

We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.

## Key Contributions

- Bidirectional pretraining approach
- Masked Language Model (MLM) objective
- Next Sentence Prediction (NSP) task
- State-of-the-art results on GLUE benchmark

## My Notes

### Main Findings

Currently reading and taking notes...

## Links

- [üìÑ Original Paper](https://arxiv.org/abs/1810.04805)
- [üìù Annotated PDF](#)