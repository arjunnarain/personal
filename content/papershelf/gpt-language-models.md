---
title: "Language Models are Unsupervised Multitask Learners"
date: 2025-08-31
draft: false
tags: ["gpt", "language-models", "unsupervised-learning"]
categories: ["research-papers"]
reading_status: ["want-to-read"]
author: "Radford et al."
journal: "OpenAI"
year: "2019"
doi: ""
paper_url: "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
drive_link: "#"
summary: "GPT-2 demonstrates that language models can perform multiple tasks without explicit supervision when trained at sufficient scale."
ShowToc: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowShareButtons: true
---

## Abstract

Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets.

## Key Contributions

- Zero-shot task transfer capabilities
- Scaling laws for language models
- WebText dataset creation

## My Notes

*Added to reading list - plan to read this month*

## Links

- [üìÑ Original Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [üìù Annotated PDF](#)